[package]
name = "rain-chat-v2"
version = "2.0.0"
description = "Professional Desktop AI IDE with Native Performance"
authors = ["RAIN.CHAT Team"]
license = "MIT"
repository = "https://github.com/AfyKirby1/RAIN.CHAT"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[build-dependencies]
tauri-build = { version = "2.0", features = [] }

[dependencies]
# Tauri framework
tauri = { version = "2.0", features = [] }
tauri-plugin-dialog = "2.4"

# Async runtime
tokio = { version = "1.0", features = ["full"] }
futures = "0.3"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Database
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono", "uuid"] }

# File system operations
tokio-util = "0.7"
walkdir = "2.3"
glob = "0.3"

# AI/ML dependencies (optional by default to avoid heavy toolchain requirements)
candle-core = { version = "0.3", optional = true }
candle-nn = { version = "0.3", optional = true }
candle-transformers = { version = "0.3", optional = true }
candle-onnx = { version = "0.3", optional = true }
hf-hub = { version = "0.3", features = ["tokio"] }
tokenizers = "0.15"

# GGUF support via llama.cpp bindings (optional to avoid build requirements by default)
llama-cpp-2 = { version = "0.1", optional = true }

# HTTP client for model downloads
reqwest = { version = "0.11", features = ["json", "stream"] }

# Process management
sysinfo = "0.30"
which = "4.4"

# Terminal/Shell (simplified)
# Note: Full terminal integration would require additional setup

# Git operations
git2 = "0.18"

# Language Server Protocol
tower-lsp = "0.20"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Utilities
uuid = { version = "1.0", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
regex = "1.10"
base64 = "0.21"
sha2 = "0.10"
hex = "0.4"

# Configuration
config = "0.14"
dirs = "5.0"

# Path handling
pathdiff = "0.2"

# Memory management
bytes = "1.5"

# Compression
flate2 = "1.0"
tar = "0.4"

# Platform specific
[target.'cfg(windows)'.dependencies]
winapi = { version = "0.3", features = ["winuser", "processthreadsapi", "handleapi", "synchapi"] }

[target.'cfg(unix)'.dependencies]
nix = "0.27"

[features]
default = ["custom-protocol"]
custom-protocol = ["tauri/custom-protocol"]
# Enable GGUF backend (llama.cpp) only when explicitly requested
gguf = ["llama-cpp-2"]

# Enable Candle-based backends when requested
ai_candle = ["candle-core", "candle-nn", "candle-transformers"]
ai_onnx = ["ai_candle", "candle-onnx"]

# Development features
[dev-dependencies]
tempfile = "3.8"
